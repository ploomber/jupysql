{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b59b49b",
   "metadata": {},
   "source": [
    "# Schedule ETLs with Jupysql and GitHub actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441f540-7739-4f4e-a417-883ac6c22291",
   "metadata": {},
   "source": [
    "![syntax](../static/syntax-highlighting-working.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03d3a6-6d1d-4a56-9a81-a197c5cf20ef",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this brief yet informative guide, we aim to provide you with a comprehensive \n",
    "understanding of the fundamental concepts of ETL (Extract, Transform, Load) and Jupysql, \n",
    "a flexible and versatile tool that allows for seamless SQL based ETL from Jupyter. \n",
    "\n",
    "Our primary focus will be on demonstrating how to effectively execute ETLs through \n",
    "JupySQL, the popular and powerful Python library designed for SQL interaction, \n",
    "while also highlighting the benefits of automating the ETL process through \n",
    "scheduling a full example ETL notebook via GitHub actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b1ecc-24cb-4413-bb65-2ffe7baa2899",
   "metadata": {},
   "source": [
    "### But first, what is an ETL?\n",
    "Now, let's dive into the details. `ETL` (Extract, Transform, Load) crucial process \n",
    "in data management that involves the extraction of data from various sources, \n",
    "transformation of the extracted data into a usable format, and loading the \n",
    "transformed data into a target database or data warehouse. It is an essential \n",
    "process for data analysis, data science, data integration, and data migration, among other purposes. \n",
    "On the other hand, Jupysql is a widely-used Python library that simplifies the interaction \n",
    "with databases through the power of SQL queries. By using Jupysql, data scientists \n",
    "and analysts can easily execute SQL queries, manipulate data frames, and interact \n",
    "with databases from their Jupyter notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b3d25b-4245-4519-9d67-688e2e04943f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Why ETLs are important?\n",
    "\n",
    "ETLs play a significant role in data analytics and business intelligence. \n",
    "They help businesses to collect data from various sources, including social media, \n",
    "web pages, sensors, and other internal and external systems. By doing this, \n",
    "businesses can obtain a holistic view of their operations, customers, and market trends.\n",
    "\n",
    "After extracting data, ETLs transform it into a structured format, such as a relational \n",
    "database, which allows businesses to analyze and manipulate data easily. \n",
    "By transforming data, ETLs can clean, validate, and standardize it, making it easier \n",
    "to understand and analyze.\n",
    "\n",
    "Finally, ETLs load the data into a database or data warehouse, \n",
    "where businesses can access it easily. By doing this, \n",
    "ETLs enable businesses to access accurate and up-to-date information, \n",
    "allowing them to make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856e198-52ac-4da4-9a6d-a5cb5556a9f8",
   "metadata": {},
   "source": [
    "### What is JupySQL?\n",
    "\n",
    "JupySQL (based on ipython-sql) is an extension for Jupyter notebooks that allows you \n",
    "to interact with databases using SQL queries. It provides a convenient way to access \n",
    "databases and data warehouses directly from Jupyter notebooks, allowing you to perform \n",
    "complex data manipulations and analyses.\n",
    "\n",
    "JupySQL supports multiple database management systems, including SQLite, MySQL, \n",
    "PostgreSQL, DuckDB, Oracle, Snowflake and more (check out our integrations section \n",
    "on the left to learn more). You can connect to databases using standard connection \n",
    "strings or through the use of environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c5c2e-98b4-4055-a351-407cceb77a0e",
   "metadata": {},
   "source": [
    "### Why JupySQL?\n",
    "JupySQL, a powerful tool, facilitates direct SQL query interaction with \n",
    "databases inside Jupyter notebooks. With a view to carrying out efficient \n",
    "and accurate data extraction and transformation processes, there are several \n",
    "critical factors to consider when performing ETLs via JupySQL. JupySQL provides \n",
    "users with the necessary tools to interact with data sources and perform data \n",
    "transformations with ease. To save valuable time and effort while guaranteeing \n",
    "consistency and reliability, automating the ETL process through scheduling a \n",
    "full ETL notebook via GitHub actions can be a game-changer. By utilizing \n",
    "JupySQL, users can achieve the best of both worlds, data interactivity (Jupyter) \n",
    "and ease of usage and SQL connectivity (JupySQL), thereby streamlining the data \n",
    "management process and allowing data scientists and analysts to concentrate on \n",
    "their core competencies - generating valuable insights and reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740da8bf-0989-404a-8820-7be6f008a608",
   "metadata": {},
   "source": [
    "### Getting started with JupySQL\n",
    "\n",
    "To use JupySQL, you need to install it using pip.\n",
    "You can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18e52b3-2909-4dd8-b42a-7615a6cd1f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/idomi/Documents/ploomber/jupysql/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jupysql --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc62d9-6b7c-4009-9844-027474c40cc1",
   "metadata": {},
   "source": [
    "Once installed, you can load the extension in Jupyter notebooks using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dceb5c9-7acb-409e-b983-704b97d0a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e8cc8-e35e-4e31-96a1-ac12bf583f8e",
   "metadata": {},
   "source": [
    "After loading the extension, you can connect to a database using the following command:\n",
    "\n",
    "```python\n",
    "%sql dialect://username:password@host:port/database\n",
    "```\n",
    "\n",
    "For example, to connect to a local DuckDB database, you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4157002f-4469-4c60-9e07-d5cd8841c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql duckdb://"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d460a-0d77-47b1-a412-63c9c2a3e3e7",
   "metadata": {},
   "source": [
    "## Performing ETLs using JupySQL\n",
    "\n",
    "To perform ETLs using JupySQL, we will follow the standard ETL process, which involves \n",
    "the following steps:\n",
    "\n",
    "1. Extract data\n",
    "2. Transform data\n",
    "3. Load data\n",
    "4. Extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947526d-839e-4a98-aa36-7b3364b35c5a",
   "metadata": {},
   "source": [
    "### Extract data\n",
    "To extract data using JupySQL, we need to connect to the source database and execute \n",
    "a query to retrieve the data. For example, to extract data from a MySQL database, \n",
    "we can use the following command:\n",
    "\n",
    "```python\n",
    "%sql mysql://username:password@host:port/database\n",
    "data = %sql SELECT * FROM mytable\n",
    "```\n",
    "This command connects to the MySQL database using the specified connection string \n",
    "and retrieves all the data from the \"mytable\" table. The data is stored in the \n",
    "\"data\" variable as a Pandas DataFrame.\n",
    "\n",
    "**Note**: We can also use `%%sql df <<` to save the data into the `df` variable\n",
    "\n",
    "Since we'll be running locally via DuckDB we can simply Extract a public dataset and start working immediately.\n",
    "We're going to get our sample dataset (we will work with the Penguins datasets via a csv file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a2db83c-dd20-46d7-85c2-6f22f2e11a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "_ = urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\",\n",
    "    \"penguins.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8c4b8-712c-47ca-9b96-3f3cddd985c1",
   "metadata": {},
   "source": [
    "And we can get a sample of the data to check we're connected and we can query the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace3f118-1a5f-4f19-9cbc-dd44d540983c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*  duckdb://\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>species</th>\n",
       "        <th>island</th>\n",
       "        <th>bill_length_mm</th>\n",
       "        <th>bill_depth_mm</th>\n",
       "        <th>flipper_length_mm</th>\n",
       "        <th>body_mass_g</th>\n",
       "        <th>sex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Adelie</td>\n",
       "        <td>Torgersen</td>\n",
       "        <td>39.1</td>\n",
       "        <td>18.7</td>\n",
       "        <td>181</td>\n",
       "        <td>3750</td>\n",
       "        <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Adelie</td>\n",
       "        <td>Torgersen</td>\n",
       "        <td>39.5</td>\n",
       "        <td>17.4</td>\n",
       "        <td>186</td>\n",
       "        <td>3800</td>\n",
       "        <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Adelie</td>\n",
       "        <td>Torgersen</td>\n",
       "        <td>40.3</td>\n",
       "        <td>18.0</td>\n",
       "        <td>195</td>\n",
       "        <td>3250</td>\n",
       "        <td>FEMALE</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('Adelie', 'Torgersen', 39.1, 18.7, 181, 3750, 'MALE'),\n",
       " ('Adelie', 'Torgersen', 39.5, 17.4, 186, 3800, 'FEMALE'),\n",
       " ('Adelie', 'Torgersen', 40.3, 18.0, 195, 3250, 'FEMALE')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM penguins.csv\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47431645-53be-4778-833d-8b7ca7f4b76f",
   "metadata": {},
   "source": [
    "### Transform data\n",
    "After extracting data, it's often necessary to transform it into a format that's \n",
    "more suitable for analysis. This step may include cleaning data, filtering data, \n",
    "aggregating data, and combining data from multiple sources. Here are some common \n",
    "data transformation techniques:\n",
    "\n",
    "* **Cleaning data**: Data cleaning involves removing or fixing errors, inconsistencies, \n",
    "   or missing values in the data. For example, you might remove rows with missing values, \n",
    "   replace missing values with the mean or median value, or fix typos or formatting errors.\n",
    "* **Filtering data**: Data filtering involves selecting a subset of data that meets \n",
    "   specific criteria. For example, you might filter data to only include records \n",
    "   from a specific date range, or records that meet a certain threshold.\n",
    "* **Aggregating data**: Data aggregation involves summarizing data by calculating \n",
    "   statistics such as the sum, mean, median, or count of a particular variable. \n",
    "   For example, you might aggregate sales data by month or by product category.\n",
    "* **Combining data**: Data combination involves merging data from multiple sources \n",
    "   to create a single dataset. For example, you might combine data from different \n",
    "   tables in a relational database, or combine data from different files.\n",
    "\n",
    "In JupySQL, you can use Pandas DataFrame methods to perform data transformations or native SQL. \n",
    "For example, you can use the rename method to rename columns, the dropna method to \n",
    "remove missing values, and the astype method to convert data types. I'll demonstrate how to do it either with pandas or SQL.\n",
    "\n",
    "* Note: You can use either `%sql` or `%%sql`, check out the difference between the two [here](https://jupysql.ploomber.io/en/latest/community/developer-guide.html?highlight=%25sql%20vs%20%25%25sql#magics-e-g-sql-sql-etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02219ab4-b136-4bfd-84e1-ac57f2a7bdb1",
   "metadata": {},
   "source": [
    "Here's an example of how to use Pandas and the JupySQL alternatives to transform data:\n",
    "```python\n",
    "# Rename columns\n",
    "df = data.rename(columns={'old_column_name': 'new_column_name'})  # Pandas\n",
    "%sql df << SELECT *, old_column_name AS new_column_name FROM data;  # JupySQL\n",
    "\n",
    "\n",
    "# Remove missing values\n",
    "data = data.dropna()  # Pandas\n",
    "%sql df << SELECT * FROM data WHERE column_name IS NOT NULL;  # JupySQL single column, can add conditions to all columns as needed.\n",
    "\n",
    "\n",
    "# Convert data types\n",
    "data['date_column'] = data['date_column'].astype('datetime64[ns]')  # Pandas\n",
    "%sql df << SELECT *, CAST(date_column AS timestamp) AS date_column FROM data  # Jupysql\n",
    "\n",
    "# Filter data\n",
    "filtered_data = data[data['sales'] > 1000]  # Pandas\n",
    "%%sql df << SELECT * FROM data WHERE sales > 1000;  # JupySQL\n",
    "\n",
    "# Aggregate data\n",
    "monthly_sales = data.groupby(['year', 'month'])['sales'].sum()  # Pandas\n",
    "%sql df << SELECT year, month, SUM(sales) as monthly_sales FROM data GROUP BY year, month  # JupySQL\n",
    "\n",
    "# Combine data\n",
    "merged_data = pd.merge(data1, data2, on='key_column')  # Pandas\n",
    "%sql df << SELECT * FROM data1 JOIN data2 ON data1.key_column = data2.key_column;  # JupySQL\n",
    "```\n",
    "In our example we'll use a simple transformations, in a similar manner to the above code.\n",
    "We'll clean our data from NAs and will split a column (species) into 3 individual columns (named for each species):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ace7ec7c-58a1-4681-8079-cfc8a400aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*  duckdb://\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "%%sql transformed_df <<\n",
    "SELECT *\n",
    "FROM penguins.csv\n",
    "WHERE species IS NOT NULL AND island IS NOT NULL AND bill_length_mm IS NOT NULL AND bill_depth_mm IS NOT NULL \n",
    "AND flipper_length_mm IS NOT NULL AND body_mass_g IS NOT NULL AND sex IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1b055e8-1ef3-4177-958b-43ff96f0a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the species column into classifiers\n",
    "transformed_df = transformed_df.DataFrame().dropna()\n",
    "transformed_df['mapped_species'] = transformed_df.species.map({\"Adelie\": 0, \"Chinstrap\": 1, \"Gentoo\": 2})\n",
    "transformed_df.drop(\"species\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f258d-9392-4e47-9477-93af61f4cf52",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "After transforming the data, we need to load it into a destination database or \n",
    "data warehouse. We can use ipython-sql to connect to the destination database \n",
    "and execute SQL queries to load the data. For example, to load data into a PostgreSQL \n",
    "database, we can use the following command:\n",
    "\n",
    "```python\n",
    "%sql postgresql://username:password@host:port/database\n",
    "%sql DROP TABLE IF EXISTS mytable;\n",
    "%sql CREATE TABLE mytable (column1 datatype1, column2 datatype2, ...);\n",
    "%sql COPY mytable FROM '/path/to/datafile.csv' DELIMITER ',' CSV HEADER;\n",
    "```\n",
    "\n",
    "This command connects to the PostgreSQL database using the specified connection \n",
    "string, drops the \"mytable\" table if it exists, creates a new table with the specified \n",
    "columns and data types, and loads the data from the CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c1d4d-8800-4ea3-8ca0-9e263e5b5185",
   "metadata": {},
   "source": [
    "Since our use case is using DuckDB locally we can simply save the newly created `transformed_df` into a csv file, but we can also use the snipped above to save it into our DB or DWH depending on our use case. \n",
    "\n",
    "Run the following step to save the new data as a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "173cdc0b-832b-43ff-ada8-568b5c8bb728",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.to_csv('transformed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f1fe38-709c-4c8e-84f4-6c8bdc296d00",
   "metadata": {},
   "source": [
    "We can see a new file called `transformed_data.csv` was created for us.\n",
    "In the next step we'll see how we can automate this process and consume the final file via GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5b4b4-e4cd-42da-af6e-48f7ff4cd9d1",
   "metadata": {},
   "source": [
    "## Scheduling on GitHub actions\n",
    "The last step in our process is executing the complete notebook via GitHub actions.\n",
    "To do that we can use `ploomber-engine` which lets you schedule notebooks, along with other notebook capabilities such as profiling, debugging etc. If needed we can pass external parameters to our notebook and make it a generic template. \n",
    "- Note: Our notebook file is loading a public dataset and saves it after ETL locally, we can easily change it to consume any dataset, and load it to S3, visualize the data as a dashboard and more.\n",
    "\n",
    "For our example we can use this sample ci.yml file (this is what sets the github workflow in your repository), and put it in our repository, the final file should\n",
    "be located under `.github/workflows/ci.yml`. \n",
    "\n",
    "Content of the `ci.yml` file:\n",
    "\n",
    "```yaml\n",
    "name: CI\n",
    "\n",
    "on:\n",
    "  push:\n",
    "  pull_request:\n",
    "  schedule:\n",
    "    - cron: '0 0 4 * *'\n",
    "\n",
    "# These permissions are needed to interact with GitHub's OIDC Token endpoint.\n",
    "permissions:\n",
    "  id-token: write\n",
    "  contents: read\n",
    "\n",
    "jobs:\n",
    "  report:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    - name: Set up Python ${{ matrix.python-version }}\n",
    "      uses: conda-incubator/setup-miniconda@v2\n",
    "      with:\n",
    "        python-version: '3.10'\n",
    "        miniconda-version: latest\n",
    "        activate-environment: conda-env\n",
    "        channels: conda-forge, defaults\n",
    "\n",
    "\n",
    "    - name: Run notebook\n",
    "      env:\n",
    "        PLOOMBER_STATS_ENABLED: false\n",
    "        PYTHON_VERSION: '3.10'\n",
    "      shell: bash -l {0}\n",
    "      run: |\n",
    "        eval \"$(conda shell.bash hook)\"\n",
    "        \n",
    "        # pip install -r requirements.txt\n",
    "        pip install jupysql pandas ploomber-engine --quiet\n",
    "        ploomber-engine --log-output posthog.ipynb report.ipynb\n",
    "\n",
    "    - uses: actions/upload-artifact@v3\n",
    "      if: always()\n",
    "      with:\n",
    "        name: Transformed_data\n",
    "        path: transformed_data.csv\n",
    "```\n",
    "\n",
    "In this example CI, I've also added a scheduled trigger, this job will run nightly at 4 am."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55a635",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "ETLs are an essential process for data analytics and business intelligence. \n",
    "They help businesses to collect, transform, and load data from various sources, \n",
    "making it easier to analyze and make informed decisions. JupySQL is a powerful \n",
    "tool that allows you to interact with databases using SQL queries directly in Jupyter \n",
    "notebooks. Combined with Github actions we can create powerful workflows that\n",
    "can be scheduled and help us get the data to its final stage.\n",
    "\n",
    "By using JupySQL, you can perform ETLs easily and efficiently, \n",
    "allowing you to extract, transform, and load data in a structured format while \n",
    "Github actions allocate compute and set the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001a62c-ff8e-4fb0-9437-e1614edad56b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "myst": {
   "html_meta": {
    "description lang=en": "Schedule ETL with JupySQL and GitHub actions",
    "keywords": "jupyter, jupyterlab, sql, jupysql, github, ETL, Data engineering",
    "property=og:locale": "en_US"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
